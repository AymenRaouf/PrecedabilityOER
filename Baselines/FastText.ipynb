{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000 NaN values in chapters.\n",
      "0000 Nan values in concepts.\n",
      "0000 Nan values in classes.\n",
      "0000 Nan values in episdes precedences.\n",
      "0000 Nan values in series precedences.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "main_publisher = 'OYC'\n",
    "\n",
    "script_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "path = os.path.join(script_dir, '../Data/' + main_publisher + '/')\n",
    "\n",
    "df_chapters = pd.read_csv(path + 'chapters.csv', delimiter = '|')\n",
    "df_chapters_embeddings = pd.read_csv(path + 'embeddings_fasttext.csv', delimiter = '|', index_col=0)\n",
    "df_concepts = pd.read_csv(path + 'concepts_bis.csv', delimiter = '|')\n",
    "df_concepts_embeddings = pd.read_csv(path + 'embeddings_concepts_bis.csv', delimiter = '|', index_col=0)\n",
    "df_classes = pd.read_csv(path + 'classes_bis.csv', delimiter = '|')\n",
    "df_classes_embeddings = pd.read_csv(path + 'embeddings_classes_bis.csv', delimiter = '|', index_col=0)\n",
    "df_precedences_episodes = pd.read_csv(path + 'precedences_episodes.csv', delimiter = '|')\n",
    "df_precedences_series = pd.read_csv(path + 'precedences_series.csv', delimiter = '|')\n",
    "\n",
    "df_concepts['Concept'] = df_concepts['Concept'].apply(lambda x : x.split('/')[-1])\n",
    "\n",
    "df_classes = df_classes.dropna()\n",
    "print(f'{df_chapters[\"Cid\"].isna().sum().sum():04d} NaN values in chapters.')\n",
    "print(f'{df_concepts.isna().sum().sum():04d} Nan values in concepts.')\n",
    "print(f'{df_classes.isna().sum().sum():04d} Nan values in classes.')\n",
    "print(f'{df_precedences_episodes.isna().sum().sum():04d} Nan values in episdes precedences.')\n",
    "print(f'{df_precedences_series.isna().sum().sum():04d} Nan values in series precedences.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "unique_oer_id = id_mapper(df_chapters['Cid'], 'OER')\n",
    "unique_concept_id =  id_mapper(df_concepts['Concept'], 'Concept')\n",
    "unique_class_id =  id_mapper(df_classes['Class'], 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16786])\n",
      "torch.Size([2, 16786])\n",
      "torch.Size([2, 2097])\n",
      "torch.Size([2, 423])\n",
      "torch.Size([2, 58295])\n",
      "torch.Size([2, 58295])\n"
     ]
    }
   ],
   "source": [
    "oer_covers_concept_subject = edge_construction(df1 = df_concepts, df2 = unique_oer_id, col = 'mappedID', \n",
    "                                       how = 'left', right_on = 'OER')\n",
    "oer_covers_concept_pr = edge_construction(df1 = df_concepts, df2 = unique_oer_id, col = 'PR', \n",
    "                                          how = 'right', right_on = 'OER')\n",
    "oer_covers_concept_object = edge_construction(df1 = df_concepts, df2 = unique_concept_id, col = 'mappedID', \n",
    "                                       how = 'left', right_on = 'Concept')\n",
    "\n",
    "oer_before_oer_ep_subject = edge_construction(df1 = df_precedences_episodes, df2 = unique_oer_id, col = 'mappedID', \n",
    "                                   how = 'left', left_on = 'Before', right_on = 'OER')\n",
    "oer_before_oer_ep_object = edge_construction(df1 = df_precedences_episodes, df2 = unique_oer_id, col = 'mappedID', \n",
    "                                   how = 'left', left_on = 'After', right_on = 'OER')\n",
    "oer_before_oer_sr_subject = edge_construction(df1 = df_precedences_series, df2 = unique_oer_id, col = 'mappedID', \n",
    "                                   how = 'left', left_on = 'Before', right_on = 'OER')\n",
    "oer_before_oer_sr_object = edge_construction(df1 = df_precedences_series, df2 = unique_oer_id, col = 'mappedID', \n",
    "                                   how = 'left', left_on = 'After', right_on = 'OER')\n",
    "\n",
    "concept_belongs_class_subject = edge_construction(df1 = df_classes, df2 = unique_concept_id, col = 'mappedID', \n",
    "                                   how = 'left', left_on = 'Concept', right_on = 'Concept')\n",
    "concept_belongs_class_object = edge_construction(df1 = df_classes, df2 = unique_class_id, col = 'mappedID', \n",
    "                                   how = 'left', left_on = 'Class', right_on = 'Class')\n",
    "\n",
    "oer_covers_concept = torch.stack([oer_covers_concept_subject, oer_covers_concept_object], dim = 0).long()\n",
    "oer_covers_concept_rev = torch.stack([oer_covers_concept_object, oer_covers_concept_subject], dim = 0).long()\n",
    "oer_before_oer_ep = torch.stack([oer_before_oer_ep_subject, oer_before_oer_ep_object], dim = 0).long()\n",
    "oer_before_oer_sr = torch.stack([oer_before_oer_sr_subject, oer_before_oer_sr_object], dim = 0).long()\n",
    "concept_belongs_class = torch.stack([concept_belongs_class_subject, concept_belongs_class_object], dim = 0).long()\n",
    "concept_belongs_class_rev = torch.stack([concept_belongs_class_object, concept_belongs_class_subject], dim = 0).long()\n",
    "print(oer_covers_concept.shape)\n",
    "print(oer_covers_concept_rev.shape)\n",
    "print(oer_before_oer_ep.shape)\n",
    "print(oer_before_oer_sr.shape)\n",
    "print(concept_belongs_class.shape)\n",
    "print(concept_belongs_class_rev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "model_fasttext = fasttext.load_model(\"cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original string is : Professor Jonathan Holloway :  \n",
      " “Fellow citizens, pardon me, and allow me to ask, why am I called upon to speak here today? What have I or those I represent to do with your national independence? Are the great principles of political freedom and of natural justice, embodied in that Declaration of Independence, extended to us? And am I, therefore, called upon to bring our humble offering to the national altar, and to confess the benefits, and express devout gratitude for the blessings resulting from your independence to us?  Would to God, both for your sakes and ours, that an affirmative answer could be truthfully returned to these questions. But such is not the state of the case. I say it with a sad sense of the disparity between us. I am not included within the pale of this glorious anniversary! Your high independence only reveals the immeasurable distance between us. The blessings in which you this day rejoice are not enjoyed in common. The rich inheritance of justice, liberty, prosperity, and independence bequeathed by your fathers is shared by you, not by me. The sunlight that brought life and healing to you has brought stripes and death to me. This Fourth of July is yours, not mine. You   may rejoice, I must mourn. To drag a man in fetters into the grand illuminated temple of liberty, and call upon him to join you in joyous anthems, were inhuman mockery and sacrilegious irony. Do you mean, citizens, to mock me, by asking me to speak today? What to the American slave is your Fourth of July? I answer, a day that reveals to him more than all other days of the year, the gross injustice and cruelty to which he is the constant victim. To him your celebration is a sham; your boasted liberty an unholy license; your national greatness, swelling vanity; your sounds of rejoicing are empty and heartless; your denunciations of tyrants, brass-fronted impudence; your shouts of liberty and equality, hollow mockery; your prayers and hymns, your sermons and thanksgivings, with all your religious parade and solemnity, are to him mere bombast, fraud, deception, impiety, and hypocrisy, a thin veil to cover up crimes which would disgrace a nation of savages. There is not a nation of the earth guilty of practices more shocking and bloody than are the people of these United States at this very hour.  Go where you may, search where you will, roam through all the monarchies and despotisms of the Old World, travel through South America, search out every abuse and when you have found the last, lay your facts by the side of the everyday practices of this nation, and you will say with me that, for revolting barbarity and shameless hypocrisy, America reigns without a rival.” \n",
      " \n",
      " Welcome to class.  Many of you will have recognized Frederick Douglass’s speech, delivered in Rochester, New York to abolitionist friends on July 5th, 1852.  Douglass is invited by his friends to come to Rochester on July 4th to talk about the meaning of freedom, the meaning of liberty, the meaning of this great country.  These were his friends.  He refused to come on July 4th for the reasons that you certainly heard in this excerpt–and this is a three hour long speech, I spared you two hours and fifty-eight minutes of.  It’s a brilliant speech.  But he refused to come on July 4th, because to talk about independence and liberty to a person who emancipated himself was unkind at best, certainly blind.  But he did come.  He came on July 5th, the next day, and offered and presented one of the great speeches in American letters. Now this course is about the African American experience after emancipation, from emancipation to the present.  Today, however, I’m going to lay the foundation for the course by discussing events prior to the emancipatory moment.  \n",
      " Now before doing that, telling you some more stories, I want to attend to some course details.  One of the first things some of you may have noticed \n",
      " [Points at Student]: You can come on down if you want to.  \n",
      " One of the first things you may have noticed is that there are some people back there, and there’s a camera, and I’m mic’ed.  This course is being filmed for the Open Yale Course Initiative, funded by the Hewlett-Packard foundation, one of six courses being filmed this semester.  You are not on camera, unless you kind of walk right in front of me right during lecture, which I kind of hope you won’t do.  You won’t be on camera, so don’t worry about it–and don’t try to join me on camera, you know, because I got it just at the right angle, you know, and I don’t want them filming my bad side.  Anyway, you just need to know that the camera’s there, and really, it shouldn’t be a factor in the course at all.  They promise to be unobtrusive.  \n",
      " Now, I’m not the only one teaching this course.  I’m leading it, certainly, but I’m not the only one that’s going to help you understand this experience.  Joining me in this task are seven different teaching assistants, a really quite excellent crew that I’m excited about. I want to introduce them to you rather quickly: Ruthie Gow [ph?] is the head TA. She is getting her PhD in African American Studies and American Studies, working on ethnography, cultural history and school desegregation in the South. Jane Ptolemy [ph?], there’s Jane, is in African American Studies and History as well–excuse me, History, studying race, benevolence and religion in the early national period.  Chris Johnson, African American Studies and History, studying gender, migration and migrant politics and radicalism in black diasporas.  Ruthie, Jane and Chris are leading writing intensive sections, and I’ll explain more about that in a few moments.  So those three are leading the writing intensive sections.  Brian Distalburg [ph?] in History, is studying 20 th century US culture and politics, specifically anti-defamation activism by historically marginalized groups.  Anastasia Jones, down there as well, is in History, studying gender and sexuality, mid 20 th  century US culture.  Lauren Perlman [ph?]–that’s like the TA corner.  That’s really cute–is in African American Studies and American Studies, studying social, cultural and political geography of Washington DC, with a focus on local civil rights and the rise of black power.  And Madison Moore, in American Studies, is studying the politics of beauty in the American fashion history, currently writing on Somali supermodel, Iman.  \n",
      " The books for the course–you should have the syllabus, by the way, that was by both doors–the books for the course are at Labyrinth.  The books are also on closed reserve at Bass Library.  There are movies during Week Three and during Week Eight that are not shown in class.  They’re shown in the evenings, on probably Monday and Tuesday.  Details on that will be forthcoming, of course.  There is a course website that you can find via the class’s server.  In fact, when you go there, that should be the first thing you see.  A note about the website: I will not be talking about it in this class. I will not be referencing it in this class.  The way to make best use of the website–and it’s not mandatory for this class–the way to make the best use of it though is on Sunday night, before you come to class, take the time to look at that week’s overview.  I’m not even going to promise all the links at the bottom of each page of the overview are actually active anymore.  Read the overview.  You will see in that overview the concept, the core of what I’ll be talking about that week.  Also notice at the bottom of that overview page is keywords.  I’ll be talking–I talk often about keywords in lectures.  You’ll see me reference these later on. You’ll find a lot of the same keywords on the webpage, and so you will know coming into class, if you look at this website on Sunday, or on Saturday night, since you’re not going to be doing anything else but preparing for this class, you will come in prepared to understand what I think are the most important things to know for that particular week’s worth of lectures.  So the website’s on the class’s server.  It’s a tool; it’s supplementary. Do take advantage of it. \n",
      " Course requirements: I mentioned that Ruthie, Chris and Jayne are teaching writing intensive sections.  We have two different types of ways to take this class.  You can take them through writing intensive, and therefore satisfy the distribution requirement in that area, or take them through the, quote, regular route.  They have different sets of obligations, although the course is the same as far as substance.  It’s all outlined in the syllabus, but essentially for writing intensive, you’ll be writing twenty pages of papers, a midterm paper and a term paper, with drafts along the way.  It’s not supposed to be a harder way to do the class.  It’s simply a different way, a way to bolster your writing skills.  For the other, for the regular path, you’ll be taking a midterm and a two-part exam, fairly straightforward survey course set of expectations, as far as that is concerned.  Now, with that little piece of business out of the way, let me get to the actual substance of the bulk of today’s lecture.\n",
      "The string after punctuation filter : Professor Jonathan Holloway   \n",
      " Fellow citizens pardon me and allow me to ask why am I called upon to speak here today What have I or those I represent to do with your national independence Are the great principles of political freedom and of natural justice embodied in that Declaration of Independence extended to us And am I therefore called upon to bring our humble offering to the national altar and to confess the benefits and express devout gratitude for the blessings resulting from your independence to us  Would to God both for your sakes and ours that an affirmative answer could be truthfully returned to these questions But such is not the state of the case I say it with a sad sense of the disparity between us I am not included within the pale of this glorious anniversary Your high independence only reveals the immeasurable distance between us The blessings in which you this day rejoice are not enjoyed in common The rich inheritance of justice liberty prosperity and independence bequeathed by your fathers is shared by you not by me The sunlight that brought life and healing to you has brought stripes and death to me This Fourth of July is yours not mine You   may rejoice I must mourn To drag a man in fetters into the grand illuminated temple of liberty and call upon him to join you in joyous anthems were inhuman mockery and sacrilegious irony Do you mean citizens to mock me by asking me to speak today What to the American slave is your Fourth of July I answer a day that reveals to him more than all other days of the year the gross injustice and cruelty to which he is the constant victim To him your celebration is a sham your boasted liberty an unholy license your national greatness swelling vanity your sounds of rejoicing are empty and heartless your denunciations of tyrants brassfronted impudence your shouts of liberty and equality hollow mockery your prayers and hymns your sermons and thanksgivings with all your religious parade and solemnity are to him mere bombast fraud deception impiety and hypocrisy a thin veil to cover up crimes which would disgrace a nation of savages There is not a nation of the earth guilty of practices more shocking and bloody than are the people of these United States at this very hour  Go where you may search where you will roam through all the monarchies and despotisms of the Old World travel through South America search out every abuse and when you have found the last lay your facts by the side of the everyday practices of this nation and you will say with me that for revolting barbarity and shameless hypocrisy America reigns without a rival \n",
      " \n",
      " Welcome to class  Many of you will have recognized Frederick Douglasss speech delivered in Rochester New York to abolitionist friends on July 5th 1852  Douglass is invited by his friends to come to Rochester on July 4th to talk about the meaning of freedom the meaning of liberty the meaning of this great country  These were his friends  He refused to come on July 4th for the reasons that you certainly heard in this excerptand this is a three hour long speech I spared you two hours and fiftyeight minutes of  Its a brilliant speech  But he refused to come on July 4th because to talk about independence and liberty to a person who emancipated himself was unkind at best certainly blind  But he did come  He came on July 5th the next day and offered and presented one of the great speeches in American letters Now this course is about the African American experience after emancipation from emancipation to the present  Today however Im going to lay the foundation for the course by discussing events prior to the emancipatory moment  \n",
      " Now before doing that telling you some more stories I want to attend to some course details  One of the first things some of you may have noticed \n",
      " Points at Student You can come on down if you want to  \n",
      " One of the first things you may have noticed is that there are some people back there and theres a camera and Im miced  This course is being filmed for the Open Yale Course Initiative funded by the HewlettPackard foundation one of six courses being filmed this semester  You are not on camera unless you kind of walk right in front of me right during lecture which I kind of hope you wont do  You wont be on camera so dont worry about itand dont try to join me on camera you know because I got it just at the right angle you know and I dont want them filming my bad side  Anyway you just need to know that the cameras there and really it shouldnt be a factor in the course at all  They promise to be unobtrusive  \n",
      " Now Im not the only one teaching this course  Im leading it certainly but Im not the only one thats going to help you understand this experience  Joining me in this task are seven different teaching assistants a really quite excellent crew that Im excited about I want to introduce them to you rather quickly Ruthie Gow ph is the head TA She is getting her PhD in African American Studies and American Studies working on ethnography cultural history and school desegregation in the South Jane Ptolemy ph theres Jane is in African American Studies and History as wellexcuse me History studying race benevolence and religion in the early national period  Chris Johnson African American Studies and History studying gender migration and migrant politics and radicalism in black diasporas  Ruthie Jane and Chris are leading writing intensive sections and Ill explain more about that in a few moments  So those three are leading the writing intensive sections  Brian Distalburg ph in History is studying 20 th century US culture and politics specifically antidefamation activism by historically marginalized groups  Anastasia Jones down there as well is in History studying gender and sexuality mid 20 th  century US culture  Lauren Perlman phthats like the TA corner  Thats really cuteis in African American Studies and American Studies studying social cultural and political geography of Washington DC with a focus on local civil rights and the rise of black power  And Madison Moore in American Studies is studying the politics of beauty in the American fashion history currently writing on Somali supermodel Iman  \n",
      " The books for the courseyou should have the syllabus by the way that was by both doorsthe books for the course are at Labyrinth  The books are also on closed reserve at Bass Library  There are movies during Week Three and during Week Eight that are not shown in class  Theyre shown in the evenings on probably Monday and Tuesday  Details on that will be forthcoming of course  There is a course website that you can find via the classs server  In fact when you go there that should be the first thing you see  A note about the website I will not be talking about it in this class I will not be referencing it in this class  The way to make best use of the websiteand its not mandatory for this classthe way to make the best use of it though is on Sunday night before you come to class take the time to look at that weeks overview  Im not even going to promise all the links at the bottom of each page of the overview are actually active anymore  Read the overview  You will see in that overview the concept the core of what Ill be talking about that week  Also notice at the bottom of that overview page is keywords  Ill be talkingI talk often about keywords in lectures  Youll see me reference these later on Youll find a lot of the same keywords on the webpage and so you will know coming into class if you look at this website on Sunday or on Saturday night since youre not going to be doing anything else but preparing for this class you will come in prepared to understand what I think are the most important things to know for that particular weeks worth of lectures  So the websites on the classs server  Its a tool its supplementary Do take advantage of it \n",
      " Course requirements I mentioned that Ruthie Chris and Jayne are teaching writing intensive sections  We have two different types of ways to take this class  You can take them through writing intensive and therefore satisfy the distribution requirement in that area or take them through the quote regular route  They have different sets of obligations although the course is the same as far as substance  Its all outlined in the syllabus but essentially for writing intensive youll be writing twenty pages of papers a midterm paper and a term paper with drafts along the way  Its not supposed to be a harder way to do the class  Its simply a different way a way to bolster your writing skills  For the other for the regular path youll be taking a midterm and a twopart exam fairly straightforward survey course set of expectations as far as that is concerned  Now with that little piece of business out of the way let me get to the actual substance of the bulk of todays lecture\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# initializing string\n",
    "test_str = df_chapters.Text.values[0]\n",
    "\n",
    "# printing original string\n",
    "print(\"The original string is : \" + test_str)\n",
    "\n",
    "# Removing punctuations in string\n",
    "# Using regex\n",
    "res = re.sub(r'[^\\w\\s]', '', test_str)\n",
    "\n",
    "# printing result\n",
    "print(\"The string after punctuation filter : \" + res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94540006\n",
      "2550\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "sentence_embeddings = []\n",
    "for sentences in df_chapters.Text.values:\n",
    "    word_embs = []\n",
    "    sentences = re.sub(r'[^\\w\\s]', '', sentences)\n",
    "    words = word_tokenize(sentences)\n",
    "    words = [w for w in words if not w.lower() in stop_words]\n",
    "    for word in words:\n",
    "        word_embedding = model_fasttext.get_word_vector(word)\n",
    "        word_embs.append(word_embedding)\n",
    "    if word_embs:\n",
    "        word_embs = sum(word_embs) / len(word_embs)\n",
    "    else:\n",
    "        # Handle the case when none of the words are in the model's vocabulary\n",
    "        word_embs = None\n",
    "    sentence_embeddings.append(word_embs)\n",
    "\n",
    "v1 = sentence_embeddings[0]\n",
    "v2 = sentence_embeddings[1]\n",
    "cos_sim = np.dot(v1, v2) / (np.linalg.norm(v1, 2) * np.linalg.norm(v2, 2))\n",
    "print(cos_sim)\n",
    "print(len(sentence_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "chapters_embeddings_tmp = {}\n",
    "chapters_r = range(len(df_chapters['Cid'].unique()))\n",
    "chapters_embeddings_bis = np.zeros(shape=(len(chapters_r), 768))\n",
    "for r in chapters_r:\n",
    "    chapters_embeddings_tmp[r] = [float(f) for f in sentence_embeddings[r]]\n",
    "    for a in range(len(chapters_embeddings_tmp[r])):\n",
    "            chapters_embeddings_bis[i][a] = chapters_embeddings_tmp[r][a]\n",
    "    i += 1\n",
    "chapters_embeddings = torch.from_numpy(chapters_embeddings_bis).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chapters_embeddings_tmp = {}\n",
    "concepts_embeddings_tmp = {} \n",
    "classes_embeddings_tmp = {}\n",
    "\n",
    "#chapters_r = range(len(df_chapters['Cid'].unique()))\n",
    "concepts_c = range(len(df_concepts['Concept'].unique()))\n",
    "classes_c = range(len(df_classes['Class'].unique()))\n",
    "\n",
    "#chapters_embeddings = np.zeros(shape=(len(chapters_r), 768))\n",
    "concepts_embeddings = np.zeros(shape=(len(concepts_c), 768))\n",
    "classes_embeddings = np.zeros(shape=(len(classes_c), 768))\n",
    "\n",
    "\n",
    "'''i = 0\n",
    "for r in chapters_r:\n",
    "    chapters_embeddings_tmp[r] = list(filter(None, df_chapters_embeddings['FastText'][r].strip(\"[]\\n\").replace(\"'\",\"\").split(\" \")))\n",
    "    chapters_embeddings_tmp[r] = [float(f) for f in chapters_embeddings_tmp[r]]\n",
    "    for a in range(len(chapters_embeddings_tmp[r])):\n",
    "            chapters_embeddings[i][a] = chapters_embeddings_tmp[r][a]\n",
    "    i += 1'''\n",
    "\n",
    "i = 0\n",
    "for r in concepts_c:\n",
    "    concepts_embeddings_tmp[r] = list(filter(None, df_concepts_embeddings['BERT'][r].strip(\"[]\\n\").replace(\"'\",\"\").split(\" \")))\n",
    "    concepts_embeddings_tmp[r] = [float(f) for f in concepts_embeddings_tmp[r]]\n",
    "    for a in range(len(concepts_embeddings_tmp[r])):\n",
    "            concepts_embeddings[i][a] = concepts_embeddings_tmp[r][a]\n",
    "    i += 1   \n",
    "\n",
    "i = 0\n",
    "for r in classes_c:\n",
    "    classes_embeddings_tmp[r] = list(filter(None, df_classes_embeddings['BERT'][r].strip(\"[]\\n\").replace(\"'\",\"\").split(\" \")))\n",
    "    classes_embeddings_tmp[r] = [float(f) for f in classes_embeddings_tmp[r]]\n",
    "    for a in range(len(classes_embeddings_tmp[r])):\n",
    "            classes_embeddings[i][a] = classes_embeddings_tmp[r][a]\n",
    "    i += 1\n",
    "\n",
    "#chapters_embeddings = torch.from_numpy(chapters_embeddings).to(torch.float32)\n",
    "concepts_embeddings = torch.from_numpy(concepts_embeddings).to(torch.float32)\n",
    "classes_embeddings = torch.from_numpy(classes_embeddings).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2097])\n",
      "HeteroData(\n",
      "  \u001b[1mOER\u001b[0m={\n",
      "    node_id=[2550],\n",
      "    x=[2550, 768]\n",
      "  },\n",
      "  \u001b[1mConcept\u001b[0m={\n",
      "    node_id=[6007],\n",
      "    x=[6007, 768]\n",
      "  },\n",
      "  \u001b[1mClass\u001b[0m={\n",
      "    node_id=[292],\n",
      "    x=[292, 768]\n",
      "  },\n",
      "  \u001b[1m(OER, covers, Concept)\u001b[0m={\n",
      "    edge_index=[2, 16786],\n",
      "    edge_attr=[16830]\n",
      "  },\n",
      "  \u001b[1m(Concept, rev_covers, OER)\u001b[0m={ edge_index=[2, 16786] },\n",
      "  \u001b[1m(OER, before_sr, OER)\u001b[0m={ edge_index=[2, 423] },\n",
      "  \u001b[1m(OER, before_ep, OER)\u001b[0m={ edge_index=[2, 2097] },\n",
      "  \u001b[1m(Concept, belongs, Class)\u001b[0m={ edge_index=[2, 58295] },\n",
      "  \u001b[1m(Class, rev_belongs, Concept)\u001b[0m={ edge_index=[2, 58295] }\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abazouzi/Documents/Code/PrerequisiteLearning/clara-datasets/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "data = HeteroData()\n",
    "data['OER'].node_id = torch.tensor(unique_oer_id['mappedID'].values)\n",
    "data['OER'].x = chapters_embeddings\n",
    "data['Concept'].node_id = torch.tensor(unique_concept_id['mappedID'].values)\n",
    "data['Concept'].x = concepts_embeddings\n",
    "data['Class'].node_id = torch.tensor(unique_class_id['mappedID'].values)\n",
    "data['Class'].x = classes_embeddings\n",
    "data['OER', 'covers', 'Concept'].edge_index = oer_covers_concept\n",
    "data['Concept', 'rev_covers', 'OER'].edge_index = oer_covers_concept_rev\n",
    "\n",
    "data['OER', 'covers', 'Concept'].edge_attr = oer_covers_concept_pr\n",
    "print(oer_before_oer_ep.shape)\n",
    "data['OER', 'before_sr', 'OER'].edge_index = oer_before_oer_sr\n",
    "data['OER', 'before_ep', 'OER'].edge_index = oer_before_oer_ep\n",
    "data['Concept', 'belongs', 'Class'].edge_index = concept_belongs_class\n",
    "data['Class', 'rev_belongs', 'Concept'].edge_index = concept_belongs_class_rev\n",
    "\n",
    "#data = T.ToUndirected()(data)\n",
    "data.validate()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def seed_everything(seed=0):                                                  \n",
    "       random.seed(seed)                                                            \n",
    "       torch.manual_seed(seed)                                                      \n",
    "       torch.cuda.manual_seed_all(seed)                                             \n",
    "       np.random.seed(seed)                                                         \n",
    "       os.environ['PYTHONHASHSEED'] = str(seed)                                     \n",
    "       torch.backends.cudnn.deterministic = True                                    \n",
    "       torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "678\t Edges for training\n",
      "84\t Edges for validation\n",
      "84\t Edges for testing\n"
     ]
    }
   ],
   "source": [
    "agnostic = False\n",
    "if agnostic:\n",
    "    num_val = 0.5\n",
    "    num_test = 0.5\n",
    "else:\n",
    "    num_val = 0.1\n",
    "    num_test = 0.1\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val = num_val,\n",
    "    num_test = num_test,\n",
    "    disjoint_train_ratio = 0.0,\n",
    "    neg_sampling_ratio = 1.0,\n",
    "    add_negative_train_samples = True,\n",
    "    edge_types=('OER', 'before_sr', 'OER')\n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(data)\n",
    "print(f'{len(train_data[\"OER\", \"before_sr\", \"OER\"].edge_label.detach().numpy())}\\t Edges for training')\n",
    "print(f'{len(val_data[\"OER\", \"before_sr\", \"OER\"].edge_label.detach().numpy())}\\t Edges for validation')\n",
    "print(f'{len(test_data[\"OER\", \"before_sr\", \"OER\"].edge_label.detach().numpy())}\\t Edges for testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 678) (678,)\n",
      "(2, 84) (84,)\n",
      "(2, 84) (84,)\n"
     ]
    }
   ],
   "source": [
    "X_train_index = train_data[\"OER\", \"before_sr\", \"OER\"].edge_label_index.numpy()\n",
    "Y_train = train_data[\"OER\", \"before_sr\", \"OER\"].edge_label.numpy()\n",
    "X_val_index = val_data[\"OER\", \"before_sr\", \"OER\"].edge_label_index.numpy()\n",
    "Y_val = val_data[\"OER\", \"before_sr\", \"OER\"].edge_label.numpy()\n",
    "X_test_index = test_data[\"OER\", \"before_sr\", \"OER\"].edge_label_index.numpy()\n",
    "Y_test = test_data[\"OER\", \"before_sr\", \"OER\"].edge_label.numpy()\n",
    "print(X_train_index.shape, Y_train.shape)\n",
    "print(X_val_index.shape, Y_val.shape)\n",
    "print(X_test_index.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(678, 1536) (678,)\n",
      "(84, 1536) (84,)\n",
      "(84, 1536) (84,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate([chapters_embeddings[X_train_index[0]],chapters_embeddings[X_train_index[1]]], axis=1)\n",
    "X_val = np.concatenate([chapters_embeddings[X_val_index[0]],chapters_embeddings[X_val_index[1]]], axis=1)\n",
    "X_test = np.concatenate([chapters_embeddings[X_test_index[0]],chapters_embeddings[X_test_index[1]]], axis=1)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 1536) (84,)\n"
     ]
    }
   ],
   "source": [
    "#X_val = np.concatenate([X_val, X_test], axis = 0)\n",
    "#Y_val = np.concatenate([Y_val, Y_test], axis = 0)\n",
    "print(X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - L2 :\n",
      "0.55 mean accuracy\n",
      " 0.61 max accuracy\n",
      " 0.49 min accuracy\n",
      " 0.04 standard deviation\n",
      "{'max_iter': 50, 'random_state': 0, 'solver': 'liblinear'}\n",
      "====================================================\n",
      "Decision Tree :\n"
     ]
    }
   ],
   "source": [
    "from classification import classify_cv\n",
    "\n",
    "classify_cv(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('baselines_fasttext_oyc.csv', sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clara-datasets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
